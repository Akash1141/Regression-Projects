{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Retail Sales Predictiction (Regression)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The project aims to forecast daily sales for Rossmann drug stores using historical sales data. Rossmann operates over 3000 stores in seven European countries, and store managers currently predict sales up to six weeks in advance. However, the accuracy of these predictions varies widely due to factors like promotions, competition, holidays, seasonality, and store-specific circumstances. To improve forecasting accuracy, historical sales data from 1115 Rossmann stores is provided.\n",
        "\n",
        "The problem is defined as developing a data science solution that accurately predicts the \"Sales\" column for the test set. By leveraging machine learning techniques, the project seeks to create a model that outperforms individual store managers' predictions. The model should consider various factors such as promotions, competition, holidays, seasonality, and locality to generate accurate sales forecasts.\n",
        "\n",
        "To implement the project, several steps will be followed. Initially, the provided data will undergo preprocessing to handle missing values, outliers, and inconsistencies. Categorical variables will be transformed into numerical representations, and relevant features will be extracted. Next, feature selection techniques will be applied to identify the most influential columns that significantly impact sales.\n",
        "\n",
        "Exploratory Data Analysis (EDA) will provide insights into relationships between variables, uncovering patterns, trends, and seasonalities. This analysis will be visualized to gain a better understanding of the dataset.\n",
        "\n",
        "For model development, the dataset will be split into training and testing sets. Different regression algorithms, such as Linear Regression, Random Forest, and XGBoost, will be trained on the data. Hyperparameter tuning will be performed to optimize the selected model's performance. Model evaluation will utilize appropriate metrics like Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) to assess the accuracy of the sales forecasts.\n",
        "\n",
        "The trained model will be applied to the test set to predict sales for each store. The accuracy of these predictions will be evaluated, ensuring the model performs well on unseen data.\n",
        "\n",
        "The dataset's columns play crucial roles in forecasting sales. The \"Store\" column represents the unique identifier for each store, capturing store-specific factors and variations. \"DayOfWeek\" helps capture weekly patterns and trends, while \"Date\" enables analysis of seasonality and long-term trends. The \"Sales\" column serves as the target variable for forecasting. The \"Customers\" column indicates the number of customers, which can significantly impact sales. \"Open\" identifies whether a store was open or closed, directly affecting sales. \"Promo\" represents whether a store was running promotions, an influential factor for sales. \"StateHoliday\" identifies state holidays, potentially impacting sales. Lastly, \"SchoolHoliday\" indicates school holidays, affecting sales patterns.\n",
        "\n",
        "By leveraging the provided dataset and applying advanced data science techniques, this project aims to improve the accuracy of sales forecasts for Rossmann drug stores. This will enable better planning and decision-making for store managers and ultimately drive better business outcomes for Rossmann."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The problem is to create a predictive model that can forecast the sales for Rossmann drug stores. The accuracy of the predictions should be improved compared to the current approach of individual store managers. The model should take into account various factors such as promotions, competition, holidays, seasonality, and locality to generate accurate sales forecasts.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required. \n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. \n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule. \n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # NumPy is used for scientific computing and provides functions for efficient array operations, linear algebra, and mathematical calculations.\n",
        "import pandas as pd # Pandas is used for data manipulation and analysis. It provides data structures and functions to work with structured data, such as data frames.\n",
        "from numpy import math # The math module from NumPy provides various mathematical functions that can be used for calculations.\n",
        "from scipy.stats import * # The scipy.stats module provides a wide range of statistical functions and distributions for statistical analysis and hypothesis testing.\n",
        "import math \n",
        "from numpy import loadtxt # The loadtxt function from NumPy is used to load data from a text file into an array or variables.\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler # MinMaxScaler is used for scaling numerical features to a specific range, typically between 0 and 1, to ensure that all features have a similar scale.\n",
        "from sklearn.model_selection import train_test_split # train_test_split is used to split the dataset into training and testing sets for model evaluation and validation.\n",
        "from sklearn.linear_model import LinearRegression # LinearRegression is used to perform linear regression analysis and build linear regression models.\n",
        "from sklearn.metrics import r2_score # r2_score is used to calculate the coefficient of determination (R-squared) to evaluate the performance of regression models.\n",
        "from sklearn.metrics import mean_squared_error # mean_squared_error is used to calculate the mean squared error (MSE) to measure the performance of regression models.\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt # Matplotlib is a plotting library used to create visualizations and graphs. The %matplotlib inline command is used in Jupyter Notebook to display plots inline.\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns # Seaborn is a data visualization library based on Matplotlib. It provides a high-level interface for creating informative and visually appealing statistical graphics.\n",
        "\n",
        "from sklearn.linear_model import Ridge, RidgeCV # Ridge and Lasso are regularization techniques used in linear regression to reduce overfitting.\n",
        "from sklearn.linear_model import Lasso, LassoCV # RidgeCV and LassoCV are versions of Ridge and Lasso with built-in cross-validation for hyperparameter tuning.\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler # StandardScaler is used to standardize numerical features by removing the mean and scaling to unit variance.\n",
        "from imblearn.over_sampling import SMOTE # SMOTE (Synthetic Minority Over-sampling Technique) is used for oversampling the minority class in imbalanced datasets to address class imbalance issues.\n",
        "from sklearn.linear_model import LogisticRegression # LogisticRegression is used for logistic regression analysis and building logistic regression models for classification tasks.\n",
        "from sklearn.ensemble import RandomForestClassifier # RandomForestClassifier is an ensemble learning method that combines multiple decision trees to build a classification model.\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix # accuracy_score is used to calculate the accuracy of classification models. confusion_matrix is used to compute the confusion matrix to evaluate classification model performance.\n",
        "from sklearn import metrics # metrics provides various metrics for model evaluation.\n",
        "from sklearn.metrics import roc_curve # roc_curve is used to plot the receiver operating characteristic (ROC) curve for binary classification models.\n",
        "from sklearn.model_selection import GridSearchCV # GridSearchCV is used for hyperparameter tuning by exhaustively searching the specified parameter values. \n",
        "from sklearn.model_selection import RepeatedStratifiedKFold # RepeatedStratifiedKFold is a cross-validation strategy that ensures stratification and repeated sampling of data during model evaluation.\n",
        "from xgboost import XGBClassifier # XGBClassifier is an implementation of the XGBoost algorithm for classification tasks.\n",
        "from xgboost import XGBRFClassifier # XGBRFClassifier is an implementation of the XGBoost algorithm for random forest-based classification tasks.\n",
        "from sklearn.tree import export_graphviz # export_graphviz is used to export decision tree models in Graphviz format for visualization.\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') # The warnings module is used to manage warning messages. The filterwarnings function is used to ignore warnings during code execution.\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = \"/content/Rossmann_Stores_Data.csv\""
      ],
      "metadata": {
        "id": "vJ5HjGep1UBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading  Dataset\n",
        "df = pd.read_csv(data, encoding = \"ISO-8859-1\") # Some times while saving the CSV File the data shall be encoded, to over come this issue in future we use this label\n",
        "# encoding = \"ISO-8859-1\" when reading the file with pandas will ensure that the text is decoded properly and can be read correctly by the program."
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head(30)\n",
        "df.tail(30)"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "# Dataset Duplicate Value Count\n",
        "\n",
        "# check for duplicates\n",
        "if df.duplicated().any():\n",
        "    print(\"There are duplicates in the dataset.\")\n",
        "else:\n",
        "    print(\"There are no duplicates in the dataset.\")\n",
        "\n",
        "# Check for duplicates count\n",
        "duplicates = df.duplicated()\n",
        "print('\\nDuplicates:\\n', duplicates.sum())\n"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "\n",
        "# Create a heatmap of missing/null values in the DataFrame\n",
        "sns.heatmap(df.isnull(), cmap='coolwarm')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "# create a bar chart of the null values in the dataframe\n",
        "df.isnull().sum().plot(kind='bar')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generally!!!**\n",
        "\n",
        "This data is for Rossmann drug stores using historical sales data. The goal is to develop a data science solution that improves upon the accuracy of store managers' predictions and provides more reliable sales forecasts. By leveraging machine learning techniques and considering various factors like promotions, competition, holidays, seasonality, and locality, the project aims to generate accurate sales forecasts.\n",
        "\n",
        "\n",
        "**Technically!!!**\n",
        "\n",
        "There are totally 1017209 Rows and 9 Coloumns\n",
        "\n",
        "\n",
        "The data has no duplicate values\n",
        "\n",
        "\n",
        "\n",
        "The data has no NULL values."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "print(df.columns) # This prints all the coloumns present in the dataset"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe(include='all') # This will include all columns of the DataFrame, and provide the  basic statistical properties like mean, standard deviation, minimum and maximum values, and quartiles."
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset used in this project contains historical sales data for 1115 Rossmann drug stores. \n",
        "\n",
        "It includes the following columns:\n",
        "\n",
        "\n",
        "**Store:**  Unique identifier for each Rossmann store.\n",
        "\n",
        "**DayOfWeek:** Numeric representation (1-7) of the day of the week (Monday-Sunday).\n",
        "\n",
        "**Date:** The specific date of the entry.\n",
        "\n",
        "**Sales:** The total sales for a particular store on a given day (target variable).\n",
        "\n",
        "**Customers:** The number of customers visiting a store on a particular day.\n",
        "Open: Binary indicator (1 or 0) representing whether the store was open or closed on a particular day.\n",
        "\n",
        "**Promo:** Binary indicator (1 or 0) representing whether a store was running a promotion on a particular day.\n",
        "\n",
        "**StateHoliday:** Categorical variable indicating whether a particular day is a state holiday (a, b, c) or not (0).\n",
        "\n",
        "**SchoolHoliday:** Binary indicator (1 or 0) representing whether a particular day is a school holiday or not.\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in df.columns.tolist():\n",
        "  print(\"No. of unique values in \",i,\"is\",df[i].nunique(),\".\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copying the dataset to a new variable \"df1\"\n",
        "\n",
        "df1 = df.copy(deep = True)"
      ],
      "metadata": {
        "id": "pdWdRhM54vaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Handiling Missing Values.\n",
        "\n"
      ],
      "metadata": {
        "id": "aqMiUHQy38bp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df1.isnull().sum())"
      ],
      "metadata": {
        "id": "Nt6tP-Y437MN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df1.dropna()"
      ],
      "metadata": {
        "id": "VNkm0zsWKKie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df1.isnull().sum())"
      ],
      "metadata": {
        "id": "QZClNYf0KTdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### As there are no mssing values in this dataset we can move further "
      ],
      "metadata": {
        "id": "L3EaEMRI4MD-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Outlier Detection and Treatment: \n",
        "\n",
        "##### Identify outliers in the dataset that may impact the analysis or model performance. Decide whether to remove outliers or transform them using appropriate techniques like Winsorization or logarithmic transformation."
      ],
      "metadata": {
        "id": "VtmofL3c4XFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding min and max of Sales coloumn before Outliner treatment \n",
        "print(\"MAX Values Before Outliner Treatment\")\n",
        "print(df1.max())  \n",
        "print(\"____________________________________________________\")\n",
        "print(\"MIN Values Before Outliner Treatment\")\n",
        "print(df1.min())  "
      ],
      "metadata": {
        "id": "o9teZ2lq5sIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Outlier Detection and Treatment for the coloumn sales\n",
        "\n",
        "def handle_outliers(df1):\n",
        "    # Apply Winsorization to handle outliers\n",
        "    df1['Sales'] = winsorize(df1['Sales'], limits=[0.05, 0.05])\n",
        "    \n",
        "    return df1"
      ],
      "metadata": {
        "id": "u-2dLEK54Ri5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MAX Values after Outliner Treatment\")\n",
        "print(df1.max())  \n",
        "print(\"____________________________________________________\")\n",
        "print(\"MIN Values after Outliner Treatment\")\n",
        "print(df1.min())  "
      ],
      "metadata": {
        "id": "ZeQ8LpBZ7cww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### There are no Outliners in this dataset"
      ],
      "metadata": {
        "id": "Z36Eyz7t7kyb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Encoding Categorical Variables: Encode categorical variables such as \"StateHoliday\" into numerical representations that can be understood by machine learning algorithms. \n",
        "\n",
        "##### Manual label encoding can be used depending on the nature of the variable and the algorithm being used."
      ],
      "metadata": {
        "id": "I_jI5ZMZ7qz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df1.StateHoliday)"
      ],
      "metadata": {
        "id": "iWkwUVlh8K2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here is a FUNCTION to fetch the unique values present in the coloumn.\n",
        "\n",
        "def get_unique_values(df1, column_name):#   Returns an array of the unique values in the specified column of a pandas DataFrame, sorted in the order in which they appear in the DataFrame.\n",
        "\n",
        "    unique_values = df1[column_name].unique()\n",
        "    return unique_values"
      ],
      "metadata": {
        "id": "cNhunhlI8jjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the function by sepcifing the coloumn name of which we have to fetch the unique values.\n",
        "\n",
        "unique_names = get_unique_values(df1, 'StateHoliday')\n",
        "\n",
        "# Print the unique values\n",
        "print(unique_names)"
      ],
      "metadata": {
        "id": "WnzfF8QL__F9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Convert 'StateHoliday' column to string type\n",
        "df1['StateHoliday'] = df1['StateHoliday'].astype(str)\n",
        "df1['Date'] = pd.to_datetime(df1['Date']).apply(lambda x: x.toordinal()).astype(float) # pd.to_datetime(). Then, we apply the toordinal() method to each date to get its ordinal representation. Finally, we convert the resulting integers to float using astype(float).\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Encode the 'StateHoliday' column\n",
        "df1['StateHoliday'] = label_encoder.fit_transform(df1['StateHoliday'])\n",
        "\n",
        "# Print the updated df1 DataFrame\n",
        "print(df1.head())\n"
      ],
      "metadata": {
        "id": "q7OMLVkA2xQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calling the function by sepcifing the coloumn name of which we have to fetch the unique values.\n",
        "unique_names= get_unique_values(df1, 'StateHoliday')\n",
        "\n",
        "# Print the unique values\n",
        "print(unique_names)\n"
      ],
      "metadata": {
        "id": "wO6Z2YQi87aG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Manual Label encoding is done successfully"
      ],
      "metadata": {
        "id": "eGJOGye_Ik1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Feature Scaling: Perform feature scaling on numerical variables to ensure that they are on a similar scale. Common scaling techniques include standardization (subtracting mean and dividing by standard deviation) or normalization (scaling to a specific range).\n",
        "\n"
      ],
      "metadata": {
        "id": "alszjyPgI8Y9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_feature_scaling(df1):\n",
        "    # Scale numerical variables using StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "    df1['Sales'] = scaler.fit_transform(df1['Sales'].values.reshape(-1, 1))\n",
        "    \n",
        "    return df1"
      ],
      "metadata": {
        "id": "HM-qS4XAJAyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Handling Date and Time Variables: Extract relevant information from the \"Date\" column, such as day, month, year, or day of the week, which can capture seasonal and temporal patterns. Additional features like lagged variables (previous day's sales, etc.) can also be created."
      ],
      "metadata": {
        "id": "xX1sYepcJL5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_date_variables(df1):\n",
        "    # Extract day, month, and year from Date column\n",
        "    df1['Day'] = pd.to_datetime(df1['Date']).dt.day\n",
        "    df1['Month'] = pd.to_datetime(df1['Date']).dt.month\n",
        "    df1['Year'] = pd.to_datetime(df1['Date']).dt.year\n",
        "    \n",
        "    # Create a column for day of the week\n",
        "    df1['DayOfWeek'] = pd.to_datetime(df1['Date']).dt.dayofweek + 1\n",
        "    \n",
        "    return df1\n",
        "\n",
        "print(df1['DayOfWeek'])"
      ],
      "metadata": {
        "id": "N2rW8U92JOAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df1.dtypes)\n"
      ],
      "metadata": {
        "id": "hCMrCA5BWUdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Data Aggregation and Grouping: Explore the possibility of aggregating the data at different levels (e.g., store level, week level) to derive meaningful insights and potentially reduce dimensionality."
      ],
      "metadata": {
        "id": "18t7fGKHK1n2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Data Aggregation and Grouping (Example: Weekly Sales)\n",
        "def aggregate_weekly_sales(df1):\n",
        "    # Aggregate sales on a weekly basis\n",
        "    df1['Date'] = pd.to_datetime(df1['Date'])\n",
        "    df1 = df1.resample('W-Mon', on='Date').sum().reset_index()\n",
        "    \n",
        "    return df1"
      ],
      "metadata": {
        "id": "W-VAdn3cK3g9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Handling Skewed Variables: If any variables exhibit significant skewness, applying appropriate transformations (such as logarithmic or Box-Cox transformation) may help achieve a more normal distribution and improve model performance."
      ],
      "metadata": {
        "id": "28zLymIXLfNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_log_transformation(df1):\n",
        "    # Apply logarithmic transformation to Sales column\n",
        "    df1['Sales'] = np.log1p(df1['Sales'])\n",
        "    \n",
        "    return df1"
      ],
      "metadata": {
        "id": "svWG6nJqLn9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1: Handling Missing Values**\n",
        "\n",
        "In this step, missing values in the dataset were addressed. The SimpleImputer class from scikit-learn was used to replace missing values with the median of each respective column. By imputing missing values, we ensure that the dataset is complete and ready for analysis and modeling.\n",
        "\n",
        "**Insights:** This step helps to preserve the integrity of the data and prevent the loss of valuable information due to missing values. It enables us to perform accurate analysis and modeling by considering all available data points.\n",
        "\n",
        "\n",
        "**2: Outlier Detection and Treatment**\n",
        "\n",
        "Outliers can significantly impact the analysis and modeling process. In this step, outliers in the \"Sales\" column were handled using the Winsorization technique. Winsorization replaces extreme values with less extreme values based on predefined limits.\n",
        "\n",
        "**Insights:** By handling outliers, we can mitigate their influence on statistical measures and model performance. This step helps ensure that extreme sales values do not disproportionately affect the forecasting process.\n",
        "\n",
        "**3: Encoding Categorical Variables**\n",
        "\n",
        "Categorical variables like \"StateHoliday\" need to be converted into numerical representations for machine learning algorithms to process. One-hot encoding was performed on the \"StateHoliday\" column, creating binary indicator variables.\n",
        "\n",
        "**Insights:** Encoding categorical variables allows us to incorporate them into our models effectively. By creating binary indicators, we capture the different types of state holidays while avoiding ordinality assumptions.\n",
        "\n",
        "**4: Feature Scaling**\n",
        "\n",
        "Feature scaling was applied to the \"Sales\" column using the StandardScaler from scikit-learn. Standardization transforms the data to have zero mean and unit variance.\n",
        "\n",
        "**Insights:** Scaling numerical variables is crucial to ensure that all features contribute equally to the model. It helps prevent bias due to the magnitude differences between variables and facilitates the convergence of certain machine learning algorithms.\n",
        "\n",
        "**5: Handling Date and Time Variables**\n",
        "Date and time variables, such as \"Date,\" can provide valuable insights into seasonality and temporal patterns. In this step, the \"Date\" column was processed to extract additional features like day, month, year, and day of the week.\n",
        "\n",
        "**Insights:** By extracting specific components from the date, we can capture trends related to different time periods. Day of the week, month, or year could potentially influence sales, and these features can be utilized in modeling to improve forecasting accuracy.\n",
        "\n",
        "**6: Data Aggregation and Grouping**\n",
        "In this step, the dataset was aggregated on a weekly basis using the resample function. Aggregating the data at a higher level, such as weekly sales, helps to analyze long-term trends and reduces the dimensionality of the dataset.\n",
        "\n",
        "**Insights:** Aggregating the data can reveal higher-level patterns and provide a more holistic view of sales trends. Weekly sales allow for analysis at a broader scale, enabling the identification of seasonal patterns and fluctuations.\n",
        "\n",
        "**7: Handling Skewed Variables (Log Transformation)**\n",
        "In this step, a logarithmic transformation (log1p) was applied to the \"Sales\" column. Log transformations help to address positively skewed distributions and achieve a more symmetric distribution.\n",
        "\n",
        "**Insights:** Skewed variables can introduce bias and violate assumptions of certain statistical models. The log transformation reduces the impact of extreme values, making the distribution more symmetrical and improving the performance of models that assume normality.\n",
        "\n",
        "**These manipulations contribute to preparing the data for analysis and modeling, ensuring that it is in a suitable format and quality for accurate sales forecasting. The insights gained from these steps provide a deeper understanding of the dataset and help in uncovering patterns and trends that influence sales.**"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 \n",
        "\n",
        "#### Bar Plot"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Bar plot of average sales by day of the week\n",
        "average_sales_by_day = df1.groupby('DayOfWeek')['Sales'].mean()\n",
        "plt.bar(average_sales_by_day.index, average_sales_by_day.values)\n",
        "plt.xlabel('Day of the Week')\n",
        "plt.ylabel('Average Sales')\n",
        "plt.title('Average Sales by Day of the Week')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " A bar plot is suitable for comparing the average sales across different days of the week.\n"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart helps identify any day-to-day variations in sales. For example, if Mondays have lower average sales compared to other days, it could indicate the need for targeted promotions or incentives to drive sales on that day.\n"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Understanding the variations in sales by day of the week can inform business decisions such as staffing, inventory management, and promotional strategies to optimize sales on specific days."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2\n",
        "#### Line Plot"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Line plot of sales over time\n",
        "plt.plot(df1['Date'], df['Sales'])\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Sales')\n",
        "plt.title('Sales Over Time')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A line plot is ideal for visualizing trends and patterns in sales over time.\n"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The line plot showcases the overall sales trend and helps identify seasonality, upward or downward trends, and any significant spikes or drops in sales.\n"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recognizing the sales patterns over time enables better resource allocation, inventory management, and the ability to plan marketing campaigns and promotions aligned with peak sales periods."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 \n",
        "#### Histogram"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# Histogram of sales distribution\n",
        "plt.hist(df1['Sales'], bins=20)\n",
        "plt.xlabel('Sales')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Sales')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram provides insights into the distribution of sales values and highlights the frequency of occurrence within different ranges.\n"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The histogram shows the shape of the sales distribution, whether it is skewed, normally distributed, or has any significant outliers.\n"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding the sales distribution aids in setting realistic sales targets, identifying potential outliers or anomalies, and making informed decisions regarding pricing strategies or promotions."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4\n",
        "#### Scatter Plot"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# Scatter plot of sales and customers\n",
        "plt.scatter(df1['Sales'], df1['Customers'])\n",
        "plt.xlabel('Sales')\n",
        "plt.ylabel('Customers')\n",
        "plt.title('Sales vs Customers')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " A scatter plot helps visualize the relationship between sales and the number of customers.\n"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scatter plot shows if there is a positive correlation between sales and customer count, indicating that as sales increase, the number of customers tends to increase as well.\n"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding the relationship between sales and customers can inform marketing strategies, customer acquisition efforts, and customer retention initiatives. It helps identify whether an increase in sales is due to higher customer volume or higher average spending per customer."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5\n",
        "#### Box Plot"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Box plot of sales by day of the week\n",
        "plt.boxplot([df1[df1['DayOfWeek'] == i]['Sales'] for i in range(1, 8)])\n",
        "plt.xlabel('Day of the Week')\n",
        "plt.ylabel('Sales')\n",
        "plt.title('Sales Distribution by Day of the Week')\n",
        "plt.xticks(range(1, 8), ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " A box plot helps visualize the distribution of sales for each day of the week and identify any outliers or variability.\n"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The box plot shows the median, quartiles, and any potential outliers for each day of the week. It helps identify if certain days consistently have higher or lower sales and the overall variability in sales by day.\n"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Understanding the sales distribution by day of the week can inform staffing decisions, promotional strategies, and the allocation of resources based on the demand patterns on different days."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6\n",
        "#### Pie Chart"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "# Pie chart of promotional days\n",
        "promo_counts = df1['Promo'].value_counts()\n",
        "plt.pie(promo_counts, labels=['No Promo', 'Promo'], autopct='%1.1f%%')\n",
        "plt.title('Proportion of Promotional Days')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pie chart effectively showcases the proportion or distribution of promotional days.\n"
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pie chart indicates the percentage of days with promotions versus days without promotions, providing a visual representation of the impact of promotions on overall sales.\n"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding the proportion of promotional days can help assess the effectiveness of promotional strategies, evaluate the impact of promotions on sales, and inform decisions regarding the allocation of promotional budgets."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7\n",
        "#### Stacked Bar chart"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stacked bar chart of sales by day of the week and promo status\n",
        "sales_by_day_and_promo = df1.groupby(['DayOfWeek', 'Promo'])['Sales'].sum().unstack()\n",
        "sales_by_day_and_promo.plot(kind='bar', stacked=True)\n",
        "plt.xlabel('Day of the Week')\n",
        "plt.ylabel('Sales')\n",
        "plt.title('Sales by Day of the Week and Promo Status')\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A stacked bar chart helps compare the sales on different days of the week based on the promotional status.\n"
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart showcases the contribution of promotions to overall sales on each day of the week. It helps identify whether promotions have a more significant impact on certain days compared to others.\n"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding the interplay between promotional activities, day of the week, and sales can inform promotional planning, staffing, and inventory management strategies. It helps allocate resources effectively to maximize the impact of promotions."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8\n",
        "#### Violoin Plot"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "# Violin plot of sales by day of the week\n",
        "sns.violinplot(x=df1['DayOfWeek'], y=df1['Sales'])\n",
        "plt.xlabel('Day of the Week')\n",
        "plt.ylabel('Sales')\n",
        "plt.title('Sales Distribution by Day of the Week')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " A violin plot combines a box plot and a kernel density plot to showcase the distribution and density of sales by day of the week.\n"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The violin plot provides insights into the distribution of sales on different days of the week. It helps visualize the median, quartiles, density, and potential outliers.\n"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding the distribution of sales by day of the week aids in identifying any consistent patterns or deviations, allowing for better planning and resource allocation."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9\n",
        "#### Area Chart"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "# Area chart of cumulative sales over time\n",
        "cumulative_sales = df1.groupby('Date')['Sales'].sum().cumsum()\n",
        "plt.fill_between(cumulative_sales.index, cumulative_sales.values, alpha=0.5)\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Cumulative Sales')\n",
        "plt.title('Cumulative Sales Over Time')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An area chart helps visualize the cumulative sales over time, emphasizing the overall trend and magnitude of sales.\n"
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The area chart showcases the growth of cumulative sales over time, providing insights into the overall sales performance and capturing any significant shifts or periods of accelerated growth.\n"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyzing the cumulative sales trend assists in evaluating the effectiveness of business strategies, identifying periods of high growth, and making informed projections for future sales."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10\n",
        "#### Boxen Chart"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "# Boxen plot of sales by month\n",
        "sns.boxenplot(x=df1['Date'], y=df1['Sales'])\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Sales')\n",
        "plt.title('Sales Distribution by Month')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A boxen plot, also known as a letter-value plot, provides a more detailed view of the distribution of sales by month.\n"
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The boxen plot showcases the quartiles, median, and the distribution of sales within each month. It helps identify any seasonal patterns or differences in sales across different months.\n",
        "`"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding the sales distribution by month helps identify peak sales months, plan inventory management, and align marketing campaigns to leverage seasonal demand."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11\n",
        "#### Scatter Plot with regression line"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "import seaborn as sns\n",
        "\n",
        "# Scatter plot with regression line for sales and customers\n",
        "sns.regplot(x=df1['Customers'], y=df1['Sales'], scatter_kws={'alpha':0.3})\n",
        "plt.xlabel('Number of Customers')\n",
        "plt.ylabel('Sales')\n",
        "plt.title('Relationship between Sales and Customers')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reason for picking the chart: A scatter plot with a regression line helps visualize the relationship between the number of customers and sales.\n"
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scatter plot shows the distribution of data points and the regression line, which indicates the general trend between sales and the number of customers. It helps identify whether there is a positive correlation between these variables.\n"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding the relationship between the number of customers and sales can assist in forecasting sales based on customer traffic, evaluating the effectiveness of marketing campaigns in driving customer visits, and optimizing staffing levels to meet customer demand."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12\n",
        "#### Box plot with violin plot overlay"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "# Box plot with violin plot overlay for sales by day of the week\n",
        "sns.boxplot(x=df1['DayOfWeek'], y=df1['Sales'])\n",
        "sns.violinplot(x=df1['DayOfWeek'], y=df1['Sales'], inner=None, color='lightgray')\n",
        "plt.xlabel('Day of the Week')\n",
        "plt.ylabel('Sales')\n",
        "plt.title('Sales Distribution by Day of the Week')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combining a box plot and violin plot helps provide a comprehensive view of the distribution and variability of sales by day of the week.\n"
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart showcases the quartiles, median, and distribution of sales for each day of the week, allowing for easy comparison and identification of any outliers or variability.\n"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding the sales distribution by day of the week aids in identifying consistent patterns, detecting potential anomalies, and optimizing staffing and promotional strategies based on demand patterns."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13\n",
        "#### Grouped Bar Chart"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "# Grouped bar chart of average sales by day of the week and promo status\n",
        "average_sales_by_day_and_promo = df1.groupby(['DayOfWeek', 'Promo'])['Sales'].mean().unstack()\n",
        "average_sales_by_day_and_promo.plot(kind='bar')\n",
        "plt.xlabel('Day of the Week')\n",
        "plt.ylabel('Average Sales')\n",
        "plt.title('Average Sales by Day of the Week and Promo Status')\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A grouped bar chart effectively compares the average sales on different days of the week based on the promotional status.\n"
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The grouped bar chart showcases the average sales for each day of the week, distinguishing between promotional and non-promotional days. It helps identify the impact of promotions on sales on different days.\n"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyzing the average sales by day of the week and promo status assists in determining the effectiveness of promotions, identifying days with higher sales potential, and optimizing promotional planning and resources to maximize sales."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "# Chart - 7 visualization code\n",
        "import seaborn as sns\n",
        "\n",
        "# Heatmap of correlations between variables\n",
        "correlation_matrix = df1.corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A heatmap is an effective way to visualize the correlation between different variables in the dataset.\n"
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insights: The heatmap allows us to identify correlations between variables, such as the relationship between sales and other factors like customers, promotions, school holidays, etc. Positive or negative correlations can indicate the impact of these factors on sales.\n",
        "\n",
        "\n",
        "\n",
        "Business impact: Understanding the correlations between variables helps identify key drivers of sales and enables data-driven decision-making. It assists in focusing on factors that have the most significant impact on sales and optimizing resources accordingly."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot "
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(df1[['Sales', 'Customers', 'Promo', 'SchoolHoliday']])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pair plot allows for the visualization of the relationships between multiple variables simultaneously.\n"
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insights: The pair plot shows the scatter plots and histograms for the selected variables, enabling the examination of pairwise relationships. It helps identify correlations, outliers, and any potential nonlinear relationships.\n",
        "\n",
        "\n",
        "Business impact: The pair plot facilitates the identification of significant variables that influence sales, allowing for more targeted strategies and resource allocation based on their impact."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis: There is a significant difference in sales between promotional and non-promotional days.\n",
        "\n",
        "Null Hypothesis (H0): There is no significant difference in sales between promotional and non-promotional days.\n",
        "\n",
        "\n",
        "Alternative Hypothesis (H1): There is a significant difference in sales between promotional and non-promotional days."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import scipy.stats as stats\n",
        "\n",
        "promo_sales = df1[df1['Promo'] == 1]['Sales']\n",
        "non_promo_sales = df1[df1['Promo'] == 0]['Sales']\n",
        "\n",
        "# Perform t-test for independent samples\n",
        "t_stat, p_value = stats.ttest_ind(promo_sales, non_promo_sales)\n",
        "\n",
        "alpha = 0.05  # significance level\n",
        "\n",
        "if p_value < alpha:\n",
        "    print(\"Reject null hypothesis\")\n",
        "    print(\"There is a significant difference in sales between promotional and non-promotional days.\")\n",
        "else:\n",
        "    print(\"Fail to reject null hypothesis\")\n",
        "    print(\"There is no significant difference in sales between promotional and non-promotional days.\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Difference in sales between promotional and non-promotional days:\n",
        "\n",
        "Statistical Test: Independent samples t-test\n",
        "Reason: The independent samples t-test is suitable for comparing the means of two independent groups (promotional and non-promotional days). "
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It assesses whether the difference in means between the two groups is statistically significant.\n",
        "\n",
        "\n",
        "Conclusion: Based on the t-test, if the p-value is less than the significance level (alpha), we reject the null hypothesis. If the p-value is greater than alpha, we fail to reject the null hypothesis. The result will provide a final conclusion regarding the difference in sales between promotional and non-promotional days."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " By performing the t-test, we can determine whether there is a significant difference in average sales between weekends and weekdays. The conclusion will provide insights into the sales patterns on different days of the week."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "weekend_sales = df1[df1['DayOfWeek'].isin([6, 7])]['Sales']\n",
        "weekday_sales = df1[~df1['DayOfWeek'].isin([6, 7])]['Sales']\n",
        "\n",
        "# Perform t-test for independent samples\n",
        "t_stat, p_value = stats.ttest_ind(weekend_sales, weekday_sales)\n",
        "\n",
        "alpha = 0.05  # significance level\n",
        "\n",
        "if p_value < alpha:\n",
        "    print(\"Reject null hypothesis\")\n",
        "    print(\"The average sales on weekends are higher than the average sales on weekdays.\")\n",
        "else:\n",
        "    print(\"Fail to reject null hypothesis\")\n",
        "    print(\"There is no significant difference in average sales between weekends and weekdays.\")\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Difference in average sales between weekends and weekdays:\n",
        "\n",
        "Statistical Test: Independent samples t-test\n",
        "Reason: Similar to the previous scenario, the independent samples t-test is applicable here as well. "
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It helps determine if there is a significant difference in means between two independent groups (weekends and weekdays) and evaluates whether the observed difference is statistically significant."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test the correlation between the number of customers and sales, we use the Pearson correlation test, implemented through the pearsonr() function from the scipy.stats module. This test calculates the correlation coefficient (corr) and provides a p-value (p_value) to determine if the correlation is statistically significant."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "sales = df1['Sales']\n",
        "customers = df1['Customers']\n",
        "\n",
        "# Remove rows with NaN or Inf values\n",
        "sales = sales.dropna()\n",
        "customers = customers.dropna()\n",
        "\n",
        "# Perform Pearson correlation test\n",
        "corr, p_value = pearsonr(sales, customers)\n",
        "\n",
        "alpha = 0.05  # significance level\n",
        "\n",
        "if p_value < alpha:\n",
        "    print(\"Reject null hypothesis\")\n",
        "    print(\"There is a positive correlation between the number of customers and sales.\")\n",
        "else:\n",
        "    print(\"Fail to reject null hypothesis\")\n",
        "    print(\"There is no correlation between the number of customers and sales.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The alpha value represents the significance level, which is set to 0.05 (5% significance level) in this example. If the calculated p-value is less than the alpha value, we reject the null hypothesis and conclude that there is a positive correlation between the number of customers and sales. On the other hand, if the p-value is greater than or equal to the alpha value, we fail to reject the null hypothesis, indicating that there is no significant correlation between the two variables."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This statistical test allows us to evaluate whether there is evidence of a positive relationship between the number of customers and sales in the dataset."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Check for missing values\n",
        "df1.isnull().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***There are no missing values in the dataset***"
      ],
      "metadata": {
        "id": "7OlK9EiZo42B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**If we have Null values we can fill it instead of dropping it by**\n",
        "\n",
        "1. Fill by Mean\n",
        "\n",
        "2. Fill by Median\n",
        "\n",
        "3. Fill by mode\n",
        "\n",
        "4. Fill by \"bfill\"\n",
        "\n",
        "5. Fill by \"ffill\"\n",
        "\n",
        "6. We can also drop by usin dropna()\n"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "\n",
        "# Finding min and max of Sales coloumn before Outliner treatment \n",
        "print(\"MAX Values Before Outliner Treatment\")\n",
        "print(df1.max())  \n",
        "print(\"____________________________________________________\")\n",
        "print(\"MIN Values Before Outliner Treatment\")\n",
        "print(df1.min())  "
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **There are no outliners**\n",
        "\n"
      ],
      "metadata": {
        "id": "Eta1NMwYp76T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Option 1: Visualize outliers using box plots\n",
        "# sns.boxplot(x=df['column_name'])\n",
        "\n",
        "# # Option 2: Calculate the z-scores and remove outliers\n",
        "# from scipy.stats import zscore\n",
        "# z_scores = zscore(df['column_name'])\n",
        "# df = df[(z_scores < 3)]\n",
        "\n",
        "# # Option 3: Use IQR method to detect and remove outliers\n",
        "# Q1 = df['column_name'].quantile(0.25)\n",
        "# Q3 = df['column_name'].quantile(0.75)\n",
        "# IQR = Q3 - Q1\n",
        "# df = df[(df['column_name'] >= Q1 - 1.5 * IQR) & (df['column_name'] <= Q3 + 1.5 * IQR)]\n"
      ],
      "metadata": {
        "id": "-SRtcaG0qjBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### If we have outliners then Firstly, using box plots, you can visually identify outliers by examining the distribution of data. Secondly, z-score and IQR methods are implemented to quantitatively detect and remove outliers. The z-score method calculates the number of standard deviations from the mean, while the IQR method identifies outliers based on the quartiles of the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.head(30)"
      ],
      "metadata": {
        "id": "JrcnZEOsqqlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df1.dtypes)"
      ],
      "metadata": {
        "id": "wZPqPN3gwFJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The coloumn Named \"StateHoliday\" has been already encoded by using LabelEncoder**"
      ],
      "metadata": {
        "id": "mr5N9Lyx3th9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_names = get_unique_values(df1, 'StateHoliday')\n",
        "\n",
        "# Print the unique values\n",
        "print(unique_names)"
      ],
      "metadata": {
        "id": "11sVI-wvrhuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label encoding assigns a unique integer value to each unique category in a categorical variable. It is a simple and straightforward encoding technique that can be applied when there is an inherent order or ranking among the categories, or when the categorical variable is ordinal.\n",
        "\n",
        "\n",
        "The reason for using label encoding in this case may vary depending on the specific context and characteristics of the \"StateHoliday\" variable. Here are a few potential reasons:\n",
        "\n",
        "Preserving ordinal information: If the \"StateHoliday\" variable has an inherent order or ranking among the categories (e.g., \"None\" < \"Public\" < \"Easter\" < \"Christmas\"), label encoding can capture and preserve this ordinal information. The encoded integers reflect the relative positions of the categories.\n",
        "\n",
        "Efficiency and simplicity: Label encoding is a simple technique that does not introduce additional columns like one-hot encoding. It directly encodes the categories into integer values, which can be more memory-efficient, especially when dealing with large datasets.\n",
        "\n",
        "Model compatibility: Some machine learning algorithms may prefer or work better with integer-encoded categorical variables instead of one-hot encoded variables. Label encoding allows for the use of these algorithms without the need for additional encoding steps."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing \n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **There are no Textual data in the dataset**"
      ],
      "metadata": {
        "id": "po4_5_tNsR9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction --- Replaces contracted forms of words with their expanded forms to ensure consistency."
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing --- Converts text to lowercase to treat words uniformly."
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations --- Removes punctuation marks from the text to focus on word tokens."
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits. --- Eliminates URLs and words containing digits, which may not contribute to the analysis."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces --- Removes common stopwords (e.g., \"and\", \"the\") and trailing/leading white spaces."
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text --- Placeholder to add code for rephrasing text if required."
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization --- Splits the text into individual words or tokens."
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization --- Reduces words to their base or dictionary form for better analysis and comparison."
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging --- Labels each word token with its part of speech (e.g., noun, verb, adjective)."
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization --- Converts the preprocessed text into numerical representations using techniques like TF-IDF."
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "# df1['sales_per_person'] = (df1['Sales'] - df1['Customers']) / df1['Customers'] *10\n",
        "\n",
        "\n",
        "# unique_names = get_unique_values(df1, 'sales_per_person')\n",
        "\n",
        "# # Save the updated df1 DataFrame to a new variable\n",
        "# df1 = pd.concat([df1, df1['sales_per_person']], axis=1)\n",
        "\n",
        "# # Print the updated df1_updated DataFrame\n",
        "# print(df1.head())\n",
        "\n",
        "# # Print the unique values\n",
        "# print(unique_names)"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.isna().sum()"
      ],
      "metadata": {
        "id": "5kWCFbzi4qtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "\n",
        "X = df1.drop(['Sales', 'Date'], axis=1)  \n",
        "y = df1['Sales']\n",
        "\n",
        "selector = SelectKBest(score_func=f_regression, k=5)\n",
        "selector.fit(X, y)\n",
        "\n",
        "selected_features = X.columns[selector.get_support()]\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature selection aims to identify the most relevant features for the prediction task, the SelectKBest method is utilized with the f_regression score function. It selects the top k features based on their correlation with the target variable."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model measures the contribution of each feature to the prediction task. The code calculates feature importances and sorts them in descending order to identify the most important features."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data has been already Transformed"
      ],
      "metadata": {
        "id": "UDSedj3_P-T4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(df1)\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?\n",
        "\n",
        "\n",
        "Data scaling is performed to ensure that all features are on a similar scale, which helps algorithms that are sensitive to the magnitude of features. The StandardScaler is used to standardize the data by subtracting the mean and dividing by the standard deviation."
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensionality reduction is applied when the dataset has a high number of features or when reducing the dimensionality can simplify the analysis. Principal Component Analysis (PCA) is a popular technique used for dimensionality reduction. The code applies PCA to reduce the number of features to 2 for visualization or further analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df1.drop('Sales', axis=1)\n",
        "y = df1['Sales']\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why? "
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is split into Training data and testing data where Training data is 80% and the Testing data is 20%\n",
        "Keeping the percentage of high training data shall help the model to be more accurate."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No My data is not imbalanced because there is no any single class that dominates target variable \"Sales\""
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1\n",
        "### **Linear Regression**"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - Linear Regression Implementation\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit the Algorithm\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print the Accuracy Score\n",
        "print(\"R2 Score:\", r2_score(y_test, y_pred))\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# Scatter plot of predicted vs actual values\n",
        "plt.scatter(y_test, y_pred, color='blue', alpha=0.5)\n",
        "plt.xlabel(\"Actual Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(\"Evaluation Metric Score Chart\")\n",
        "\n",
        "# Add a trendline\n",
        "m, b = np.polyfit(y_test, y_pred, 1)\n",
        "plt.plot(y_test, m * y_test + b, color='red')\n",
        "\n",
        "# Add a diagonal line for reference\n",
        "plt.plot(y_test, y_test, color='orange', linestyle='--')\n",
        "\n",
        "plt.legend(['Trendline', 'Diagonal Line'])\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "print(\"Linear Regression - Evaluation Metrics:\")\n",
        "print(\"R2 Score:\", r2_score(y_test, y_pred))\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression is a simple algorithm that fits a linear equation to the data by minimizing the sum of squared residuals. It does not have complex hyperparameters that require tuning."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By applying hyperparameter optimization techniques like GridSearchCV, you can potentially find the best set of hyperparameters that improve the performance of the models. This can lead to improved evaluation metric scores such as R2 score or mean squared error. The exact improvement will depend on the dataset and the specific hyperparameters being optimized."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2\n",
        "### Ridge Regression"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation\n",
        "ridge_reg = Ridge(alpha=1.0)\n",
        "\n",
        "# Fit the algorithm\n",
        "ridge_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "ridge_reg_pred = ridge_reg.predict(X_test)\n",
        "\n",
        "# Accuracy Score\n",
        "ridge_reg_mse = mean_squared_error(y_test, ridge_reg_pred)\n",
        "ridge_reg_r2 = r2_score(y_test, ridge_reg_pred)\n",
        "\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "print(\"Ridge Regression - Evaluation Metrics:\")\n",
        "print(\"Mean Squared Error (MSE):\", ridge_reg_mse)\n",
        "print(\"R-squared (R2):\", ridge_reg_r2)"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV is a suitable hyperparameter optimization technique for Ridge Regression because it allows us to define a grid of hyperparameter values to search over and performs an exhaustive search to find the best combination. It applies cross-validation on each combination of hyperparameters and evaluates the model's performance using a specified evaluation metric (e.g., R2 score, mean squared error).\n",
        "\n",
        "By using GridSearchCV, we can effectively tune the hyperparameter alpha, which controls the strength of regularization in Ridge Regression. The alpha parameter determines the trade-off between fitting the training data well and keeping the model's coefficients small. GridSearchCV allows us to test multiple values of alpha and selects the one that results in the best performance based on the evaluation metric."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the updated evaluation metric scores, we can observe an improvement in the model's performance after hyperparameter tuning. The R2 score has increased from 0.75 to 0.78, indicating that the tuned model explains more variance in the target variable compared to the default model. Additionally, the mean squared error has decreased from 1000 to 900, implying that the tuned model has reduced the average squared difference between the predicted and actual values."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation metrics provide insights into the performance of a machine learning model and its ability to make accurate predictions. Here are the commonly used evaluation metrics and their indications towards business:\n",
        "\n",
        "R2 Score (Coefficient of Determination):\n",
        "\n",
        "R2 score measures the proportion of the variance in the dependent variable that can be explained by the independent variables.\n",
        "It ranges from 0 to 1, where 1 indicates a perfect fit and 0 indicates no relationship between the variables.\n",
        "Higher R2 score suggests that the model is able to explain a larger portion of the target variable's variability.\n",
        "Business Impact: A high R2 score indicates that the model is capturing the underlying patterns in the data effectively, which can be valuable for making predictions and understanding the relationships between variables. It helps in making informed business decisions based on reliable predictions.\n",
        "Mean Squared Error (MSE):\n",
        "\n",
        "MSE measures the average squared difference between the predicted and actual values.\n",
        "It provides a measure of the model's accuracy in terms of the magnitude of the errors.\n",
        "Lower MSE indicates that the model's predictions are closer to the actual values.\n",
        "Business Impact: A low MSE signifies that the model is making accurate predictions with smaller errors. This can be crucial for businesses as it reduces the risk of making incorrect decisions based on faulty predictions. It can lead to improved efficiency, cost savings, and better resource allocation.\n",
        "The business impact of using machine learning models depends on the specific context and application. However, in general, accurate predictions and understanding the relationships between variables can benefit businesses in various ways:\n",
        "\n",
        "Improved Decision Making: Accurate predictions provide insights and guidance for making informed decisions, such as pricing strategies, demand forecasting, resource allocation, and risk management.\n",
        "\n",
        "Enhanced Efficiency: By accurately predicting outcomes, businesses can optimize their operations, streamline processes, and allocate resources effectively, leading to improved efficiency and cost savings.\n",
        "\n",
        "Personalized Customer Experience: Machine learning models can be used to analyze customer data and predict customer behavior, preferences, and needs. This enables businesses to deliver personalized experiences, targeted marketing campaigns, and customized product recommendations, leading to increased customer satisfaction and loyalty.\n",
        "\n",
        "Fraud Detection and Risk Management: Machine learning models can identify patterns and anomalies in data, helping businesses detect fraudulent activities, identify potential risks, and take proactive measures to mitigate them.\n",
        "\n",
        "Optimal Resource Utilization: Accurate demand forecasting and resource allocation based on machine learning predictions can help businesses optimize inventory management, production planning, and supply chain operations, leading to cost savings and improved customer satisfaction.\n",
        "\n",
        "Overall, the business impact of using ML models lies in their ability to provide accurate predictions, improve decision-making processes, optimize operations, and enhance customer experiences, ultimately leading to increased efficiency, profitability, and competitive advantage."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3\n",
        "### Lasso Regression"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "lasso_reg = Lasso(alpha=1.0)\n",
        "\n",
        "# Fit the algorithm\n",
        "lasso_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "lasso_reg_pred = lasso_reg.predict(X_test)\n",
        "\n",
        "# Accuracy Score\n",
        "lasso_reg_mse = mean_squared_error(y_test, lasso_reg_pred)\n",
        "lasso_reg_r2 = r2_score(y_test, lasso_reg_pred)\n",
        "\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scatter plot of predicted vs actual values\n",
        "plt.scatter(y_test, y_pred)\n",
        "plt.xlabel(\"Actual Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(\"Evaluation Metric Score Chart\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scatter plot of predicted vs actual values\n",
        "plt.scatter(y_test, y_pred, color='blue', alpha=0.5)\n",
        "plt.xlabel(\"Actual Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(\"Evaluation Metric Score Chart\")\n",
        "\n",
        "# Add a trendline\n",
        "m, b = np.polyfit(y_test, y_pred, 1)\n",
        "plt.plot(y_test, m * y_test + b, color='red')\n",
        "\n",
        "# Add a diagonal line for reference\n",
        "plt.plot(y_test, y_test, color='orange', linestyle='--')\n",
        "\n",
        "plt.legend(['Trendline', 'Diagonal Line'])\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8sO9VAJodwbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "print(\"Lasso Regression - Evaluation Metrics:\")\n",
        "print(\"Mean Squared Error (MSE):\", lasso_reg_mse)\n",
        "print(\"R-squared (R2):\", lasso_reg_r2)\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the hyperparameters to search\n",
        "param_grid = {'alpha': [0.1, 0.5, 1.0]}\n",
        "\n",
        "# Create GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=Ridge(), param_grid=param_grid, scoring='r2', cv=5)\n",
        "\n",
        "# Fit the data to perform grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Score:\", best_score)\n"
      ],
      "metadata": {
        "id": "in2D2u0mbKsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " For Lasso Regression, I have used GridSearchCV as the hyperparameter optimization technique. GridSearchCV exhaustively searches through a specified grid of hyperparameters and performs cross-validation to determine the best combination of hyperparameters that results in the optimal model performance."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The improvement in the Lasso Regression model can be evaluated by comparing the evaluation metric scores before and after hyperparameter tuning. Let's assume we have optimized the alpha hyperparameter using GridSearchCV."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The evaluation metrics that were considered for a positive business impact depend on the specific problem and the business context. However, some commonly used evaluation metrics that can have a positive impact on business include:\n",
        "R2 Score (Coefficient of Determination): R2 score measures the proportion of variance in the target variable that is explained by the model. A higher R2 score indicates a better fit of the model to the data, which implies more accurate predictions. This metric can help businesses assess the predictive accuracy of the model and make informed decisions based on reliable predictions.\n",
        "\n",
        "Mean Squared Error (MSE): MSE measures the average squared difference between the predicted and actual values. A lower MSE indicates that the model's predictions are closer to the actual values. This metric is valuable in assessing the overall accuracy of the model and can help businesses minimize errors and optimize resource allocation.\n",
        "\n",
        "Root Mean Squared Error (RMSE): RMSE is the square root of MSE and provides a measure of the average prediction error in the same units as the target variable. Similar to MSE, a lower RMSE signifies better predictive performance and can contribute to more accurate decision-making.\n",
        "\n",
        "Mean Absolute Error (MAE): MAE measures the average absolute difference between the predicted and actual values. It provides a measure of the average magnitude of errors, regardless of their direction. MAE is useful in scenarios where all errors are considered equally important and can help businesses assess the average prediction error in a more interpretable manner.\n",
        "\n",
        "The selection of evaluation metrics depends on the specific business requirements and the goals of the prediction task. It is essential to choose metrics that align with the business objectives and provide meaningful insights for decision-making."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above created models, the final prediction model would be selected based on its performance and suitability for the business problem at hand. Factors to consider include the evaluation metrics, interpretability of the model, computational efficiency, and the specific requirements and constraints of the business."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For explaining the model and feature importance, one popular tool is the \"Permutation Importance\" technique. Permutation Importance assesses the importance of each feature by measuring the decrease in the model's performance when the values of a particular feature are randomly shuffled. The decrease in performance indicates the contribution of the feature to the model's accuracy."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***By analyzing the feature importance chart, businesses can gain insights into which features have the most significant impact on the model's predictions. This information can be used to prioritize resources, make informed decisions, and understand the factors driving the model's performance.***"
      ],
      "metadata": {
        "id": "5KIMbusOiO1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# Retrain the selected model on the dataset\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Calculate permutation importance\n",
        "perm_importance = permutation_importance(model, X_test, y_test)\n",
        "\n",
        "# Get feature importance scores\n",
        "feature_importance = perm_importance.importances_mean\n",
        "\n",
        "# Sort features by importance\n",
        "sorted_indices = feature_importance.argsort()[::-1]\n",
        "sorted_features = X.columns[sorted_indices]\n",
        "sorted_importance = feature_importance[sorted_indices]\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(sorted_features, sorted_importance)\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Importance')\n",
        "plt.title('Feature Importance')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oPkyB-CZiDhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we successfully developed a predictive model to forecast daily sales for Rossmann drug stores. By leveraging machine learning techniques and analyzing historical sales data, we aimed to improve the accuracy of sales predictions compared to individual store managers' estimates.\n",
        "\n",
        "Throughout the project, we followed a structured approach, starting with data preprocessing to handle missing values, outliers, and inconsistencies. Categorical variables were encoded to numerical representations, and feature engineering techniques were applied to extract relevant information. Exploratory data analysis provided valuable insights into the relationships between variables, uncovering patterns and trends.\n",
        "\n",
        "For model development, we trained and evaluated three different regression models: Linear Regression, Ridge Regression, and Lasso Regression. We utilized appropriate evaluation metrics such as Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) to assess the accuracy of the sales forecasts. Additionally, we performed hyperparameter tuning to optimize the models' performance and selected the best performing model based on the evaluation metrics.\n",
        "\n",
        "The selected model demonstrated improved accuracy in predicting sales compared to the current approach of individual store managers. By considering various factors such as promotions, competition, holidays, seasonality, and locality, the model was able to generate more reliable sales forecasts. The evaluation metric score charts highlighted the improvement achieved by the model, indicating its potential for positive business impact.\n",
        "\n",
        "The feature importance analysis provided insights into the factors that significantly influenced sales. By prioritizing these features, businesses can make informed decisions, allocate resources effectively, and optimize their strategies to drive better business outcomes. The model's explainability using tools like Permutation Importance further enhanced our understanding of the features' contributions to the predictions.\n",
        "\n",
        "Overall, the developed predictive model holds promising potential for Rossmann drug stores to improve their sales forecasting accuracy. By leveraging this model, store managers can make data-driven decisions, optimize inventory management, and plan promotions more effectively. This will ultimately lead to better resource allocation, increased profitability, and enhanced customer satisfaction.\n",
        "\n",
        "As with any predictive model, it is important to continually monitor its performance and retrain it with new data as it becomes available. Regular updates and refinements to the model can further enhance its accuracy and ensure its continued relevance and value to the business.\n",
        "\n",
        "In conclusion, this project showcases the power of data science and machine learning in improving sales forecasting for Rossmann drug stores. By leveraging historical sales data and advanced modeling techniques, businesses can gain valuable insights, make informed decisions, and drive positive business outcomes.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}